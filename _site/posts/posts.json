[
  {
    "path": "posts/2021-09-02-in-class-exercise-3/",
    "title": "In-Class Exercise 03",
    "description": "The topic of today's in-class exercise is analytical mapping! We'll learn about the different type of maps and how to create functions that'll help us with the analysis üòÑ",
    "author": [
      {
        "name": "Megan Sim",
        "url": "https://www.linkedin.com/in/megan-sim-tze-yen/"
      }
    ],
    "date": "2021-09-02",
    "categories": [
      "In-Class Exercise",
      "R",
      "sf"
    ],
    "contents": "\r\n\r\nContents\r\n1.0 Overview\r\n2.0 Writing Functions\r\n3.0 Extreme Value Visaulisations\r\n3.1 Boxplots\r\n3.2 Percentile Maps\r\n3.3 Box Maps\r\n3.4 Rate Maps\r\n\r\n\r\n\r\n\r\n\r\n1.0 Overview\r\nBefore we start, you might be thinking, ‚ÄúWait, don‚Äôt we already know how to create choropeth maps from Hands-On Exercise 03? What else do we need?‚Äù It‚Äôs true that choropeth maps are a great statistical representation of our data, but they are just as often misrepresented or misinterpreted. For example, take the following:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThese maps are using the same data, but they look different because of their data classification methods: the map on the left uses the quantile classification method while the one on the right uses the equal interval classification method. As a result, the viewer would have different takeaways from viewing the maps:\r\n\r\n\r\nThis, of course, has far-reaching implications and real-world consequences. For example, in the context of the ongoing global pandemic COVID-19, local authorities are concerned about the senior citizens in our country who are more susceptible and have a higher risk factor. As such, they need to find out which subzones have extremely high numbers of aged population, and roll out more measures in said subzones - perhaps more check-ins, higher priority on particular resources etc. that would undoubtedly have life-saving consequences.\r\nFrom the maps above, they can either choose to focus all their attention on one particular subzone (equal classification), or they might spread their resources out over a wide area, with a particular focus on central-to-east subzones (quantile classification). But how do we know which one is correct? Focusing all their efforts in one sole subzone could lead to missing out on other subzones that might have a middling amount of senior citizens but a high concentration - affecting the lives of thousands, or tens or thousands. Having their focus on many subzones across the whole island, on the other hand, might mean spreading their resources too thin - which has similar negative effects on tens of thousands of civilian lives.\r\nTo get an accurate understanding of the data, and to truthfully display that - this is the crux of the issue that we face when creating visualisations. And the best way to do that is to have a wide variety of visualisations on hand - which is what we‚Äôll be learning in this week‚Äôs in-class exercise!\r\n2.0 Writing Functions\r\nLet‚Äôs write functions! They‚Äôre preferred over copy-and-pasting your past formulae: you can update your function as requirements change instead of needing to update the copy-pasted code one-by-one, AND reduce the risk of human error (read: mistakes!) that comes with copy-pasting code, such as forgetting to update a variable name.\r\nIn this section, we‚Äôd like to write a function to extract a variable as a vector. We presently have our mpsz2020 data - let‚Äôs clean it up a little and check for its summary statistics:\r\n\r\n\r\nmpszpop2020a <- mpszpop2020 %>%\r\n  drop_na()\r\n\r\npercent <- c(0,.01,.1,.5,.9,.99,1)\r\nvar <- mpszpop2020a[\"DEPENDENCY\"] %>%\r\n  st_set_geometry(NULL)\r\nquantile(var[,1], percent)\r\n\r\n\r\n        0%         1%        10%        50%        90%        99% \r\n 0.0000000  0.1377778  0.5686120  0.7024793  0.8474114  1.2100000 \r\n      100% \r\n19.0000000 \r\n\r\nNow, let‚Äôs actually create the function!\r\n\r\n\r\nget.var <- function(vname,df) {\r\n  v <- df[vname] %>% \r\n    st_set_geometry(NULL)\r\n  v <- unname(v[,1])\r\n  return(v)\r\n}\r\n\r\n\r\n\r\n\r\n\r\nWe‚Äôll be using this in Section 3.2 Percentile Maps, so keep it in the back of your mind! üí™\r\n3.0 Extreme Value Visaulisations\r\nThere are many ways we can visualise extreme values, and in this section we‚Äôll learn about boxplots and extreme value maps! Extreme value maps help ‚Äòspatialize EDA‚Äô, and are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers.\r\n\r\n\r\n3.1 Boxplots\r\nBoxplots are a great way to show the statistics & distribution of data values - and especially to look at outliers!\r\n\r\n\r\n\r\nHowever, a downside is that they‚Äôre unable to reveal the spatial distribution of said outliers - which is why we use them primarily for EDA and turn to maps for visualisation when it comes to geospatial data.\r\n3.2 Percentile Maps\r\n3.2.1 Introduction to Percentile Maps\r\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. Also, remember the function we wrote in 2.0 Writing Functions? We‚Äôll be putting that to good use now!\r\n\r\n\r\npercent <- c(0,.01,.1,.5,.9,.99,1)\r\nvar <- get.var(\"DEPENDENCY\", mpszpop2020a)\r\nbperc <- quantile(var,percent)\r\ntm_shape(mpszpop2020) +\r\n  tm_polygons() +\r\ntm_shape(mpszpop2020a) +\r\n  tm_fill(\"DEPENDENCY\",\r\n          title=\"DEPENDENCY\",\r\n          breaks=bperc,\r\n          palette=\"Blues\",\r\n          labels=c(\"< 1%\", \"1% - 10%\",\r\n                   \"10% - 50%\", \r\n                   \"50% - 90%\",\r\n                   \"90% - 99%\", \r\n                   \"> 99%\"))  +\r\n  tm_borders() +\r\n  tm_layout(title = \"Percentile Map\", \r\n            title.position = c(\"right\",\r\n                               \"bottom\"))\r\n\r\n\r\n\r\n\r\n3.2.2 Writing a percentmap function\r\nThat looks good! But, having to write this every time we want a map is taxing‚Ä¶ so let‚Äôs put what we learned in 2.0 Writing Functions to use, and write a handy percent map function that‚Äôll visualise a percentile map for whichever variable we‚Äôd like ‚ú®\r\n\r\n\r\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\r\n  percent <- c(0,.01,.1,.5,.9,.99,1)\r\n  var <- get.var(vnam,df)\r\n  bperc <- quantile(var,percent)\r\n  tm_shape(mpszpop2020) +\r\n  tm_polygons() +\r\n  tm_shape(df) +\r\n     tm_fill(vnam,\r\n             title=legtitle,\r\n             breaks=bperc,\r\n             palette=\"Blues\",\r\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\r\n  tm_borders() +\r\n  tm_layout(title = mtitle, title.position = c(\"right\",\"bottom\"))\r\n}\r\n\r\n\r\n\r\nLet‚Äôs test out our function:\r\n\r\n\r\nyoungmap <- percentmap(\"YOUNG\", mpszpop2020a)\r\nagedmap <- percentmap(\"AGED\", mpszpop2020a)\r\n\r\n\r\n\r\n\r\n\r\n\r\n3.3 Box Maps\r\nA box map displays summary statistics on a choropleth map by using the basic principles of boxplot. We‚Äôll use a custom breaks specification when creating a box map - though note that these vary depending on whether lower or upper outliers are present.\r\n3.3.1 Writing a boxbreaks function\r\nLet‚Äôs create a function that‚Äôll create break points for our boxmap! It needs to consider if there are lower/upper outliers, and returns us a vector of breakpoints.\r\n\r\n\r\nboxbreaks <- function(v,mult=1.5) {\r\n  qv <- unname(quantile(v))\r\n  iqr <- qv[4] - qv[2]\r\n  upfence <- qv[4] + mult * iqr\r\n  lofence <- qv[2] - mult * iqr\r\n  # initialize break points vector\r\n  bb <- vector(mode=\"numeric\",length=7)\r\n  # logic for lower and upper fences\r\n  if (lofence < qv[1]) {  # no lower outliers\r\n    bb[1] <- lofence\r\n    bb[2] <- floor(qv[1])\r\n  } else {\r\n    bb[2] <- lofence\r\n    bb[1] <- qv[1]\r\n  }\r\n  if (upfence > qv[5]) { # no upper outliers\r\n    bb[7] <- upfence\r\n    bb[6] <- ceiling(qv[5])\r\n  } else {\r\n    bb[6] <- upfence\r\n    bb[7] <- qv[5]\r\n  }\r\n  bb[3:5] <- qv[2:4]\r\n  return(bb)\r\n}\r\n\r\n\r\n\r\nNow, let‚Äôs test it out!\r\n\r\n\r\nmpszpop2020a <- mpszpop2020 %>%\r\n  filter(AGED>=0)\r\nvar <- get.var(\"AGED\", mpszpop2020a)\r\nboxbreaks(var)\r\n\r\n\r\n[1] -4330     0   515  2080  3745  8590 20240\r\n\r\n3.3.2 Writing a boxmap function\r\nNow that we have our boxbreaks function, we‚Äôll need to map it out! Like in 3.1 Percentile Maps, let‚Äôs make a handy function to help us:\r\n\r\n\r\nboxmap <- function(vnam, df, \r\n                   legtitle=NA,\r\n                   mtitle=\"Box Map\",\r\n                   mult=1.5){\r\n  var <- get.var(vnam,df)\r\n  bb <- boxbreaks(var)\r\n  tm_shape(mpszpop2020) +\r\n    tm_polygons() +\r\n  tm_shape(df) +\r\n     tm_fill(vnam,title=legtitle,\r\n             breaks=bb,\r\n             palette=\"Blues\",\r\n          labels = c(\"lower outlier\", \r\n                     \"< 25%\", \r\n                     \"25% - 50%\", \r\n                     \"50% - 75%\",\r\n                     \"> 75%\", \r\n                     \"upper outlier\"))  +\r\n  tm_borders() +\r\n  tm_layout(title = mtitle, \r\n            title.position = c(\"right\",\r\n                               \"bottom\"))\r\n}\r\n\r\n\r\n\r\n\r\n\r\nLet‚Äôs test it out:\r\n\r\n\r\nboxmap(\"ECONOMY ACTIVE\", mpszpop2020a)\r\n\r\n\r\n\r\n\r\nFrom the map, we can see that there are 6 upper outliers - in context of our data, there are 6 planning subzones with extremely high numbers of aged population, and out of those 6, 4 of them are clustered around the Eastern region.\r\n\r\nNote: the inclusion of tm_shape(mpszpop2020) allows the map to keep its shape, even if there are areas with no/missing data! Without it, your map would look weirdly potato-shaped, like so:\r\n\r\n\r\n\r\n\r\n\r\n\r\npotatoboxmap(\"ECONOMY ACTIVE\", mpszpop2020a)\r\n\r\n\r\n\r\n\r\n3.4 Rate Maps\r\n\r\n\r\nAs our friends at xkcd put it, the population is not evenly distributed across the map. As such, counting-based map distributions maps roughly start representing the population instead of our topic of interest For example:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis is where a rate map comes in! Let‚Äôs computer the raw rate first:\r\n\r\n\r\nmpszpop2020rate <- mpszpop2020 %>%\r\n  mutate(`AGED%` = (`AGED`\r\n/`TOTAL`)*100) %>%\r\n  filter(`AGED%` >= 0)\r\n\r\n\r\n\r\nAnd use our handy boxmap function to plot to raw rate map:\r\n\r\n\r\nvar <- get.var(\"AGED%\", mpszpop2020rate)\r\nboxbreaks(var)\r\n\r\n\r\n[1] -2.17276  0.00000 11.28169 16.48199 20.25132 33.70576 95.00000\r\n\r\nboxmap(\"AGED%\",mpszpop2020rate)\r\n\r\n\r\n\r\n\r\nNow we can see the difference:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-09-03T06:30:13+08:00",
    "input_file": "in-class-exercise-3.knit.md"
  },
  {
    "path": "posts/2021-09-02-hands-on-exercise-3/",
    "title": "Hands-On Exercise 3",
    "description": "Today's Adventure: Thematic & Analytical Mapping! With the tmap package, let's learn how to plot functional and truthful choropleth maps üí™",
    "author": [
      {
        "name": "Megan Sim",
        "url": "https://www.linkedin.com/in/megan-sim-tze-yen/"
      }
    ],
    "date": "2021-09-02",
    "categories": [
      "In-Class Exercise",
      "R",
      "sf",
      "tmap",
      "tidyverse"
    ],
    "contents": "\r\n\r\nContents\r\n0.0 WIP\r\n1.0 Overview\r\n2.0 Setup\r\n2.1 Packages Used\r\n2.2 Data Used\r\n2.3 Importing Data\r\n2.4 Data Preparation\r\n\r\n3.0 Choropleth Maps\r\n\r\n0.0 WIP\r\nWORK IN PROGRESS - will be finished by 5th sept 2021!\r\nIn the meantime, would you like to check out my Hands-On Exercise 02? It‚Äôs got article references and hand-drawn illustrations, and I worked really hard on it so I‚Äôd love if you could take a look! ü§ó\r\n1.0 Overview\r\nIn Hands-On Exercise 02, we learned how to handle geospatial data in R: from importing to content-checking to plotting and projection! We ever learned a couple of geoprocessing tricks üòâ. Now that we know how to go all of that, let‚Äôs get into the meat of geospatial visualisations: choropleth maps!\r\nSo what are choropleth maps? Here‚Äôs a great primer on what they are (with examples!) as well as the advantages + disadvantages of using them - but in short, they‚Äôre thematic maps: maps used to represent statistical data of a geographic region - usually through patterns or graduated colours. For example, seeing the distribution of elderly across Singapore‚Äôs various subzones!\r\n\r\n\r\n2.0 Setup\r\n2.1 Packages Used\r\nThe R packages we‚Äôll be introducing today are: - tmap: used for creating thematic maps, such as choropleth and bubble maps\r\nIn addition, we‚Äôll be using the packages from our last lesson: - sf: used for importing, managing, and processing geospatial data - tidyverse: used for importing, wrangling and visualising data (and other data science tasks!)\r\n\r\n\r\npackages = c('sf', 'tmap', 'tidyverse')\r\nfor (p in packages){\r\n  if(!require(p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p,character.only = T)\r\n}\r\n\r\n\r\n\r\n2.2 Data Used\r\nThe datasets used for this exercise are:\r\nMaster Plan 2014 Subzone Boundary (Web) (MP14_SUBZONE_WEB_PL) from data.gov.sg\r\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 (respopagesextod2011to2020.csv) from Department of Statistics, Singapore\r\n\r\nNote: Our aspatial data file does not contain any coordinates values, but it‚Äôs PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile!\r\n\r\n2.3 Importing Data\r\nWe‚Äôve learned how to import data in Hands-On Exercise 02, so try doing it yourself in this section! If you‚Äôre stuck, click on ‚Äúshow code‚Äù below.\r\n2.3.1 Importing Geospatial Data\r\n\r\n\r\nShow code\r\n\r\nmpsz <- st_read(dsn = \"data/geospatial\", \r\n                layer = \"MP14_SUBZONE_WEB_PL\")\r\n\r\n\r\nReading layer `MP14_SUBZONE_WEB_PL' from data source \r\n  `C:\\Users\\megan\\relevant\\University Materials\\IS415 Geospatial Analytics & Applications\\RStudio\\IS415_msty\\_posts\\2021-09-02-hands-on-exercise-3\\data\\geospatial' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 323 features and 15 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\r\nProjected CRS: SVY21\r\n\r\nLet‚Äôs examine the content - note that only the first 10 records are displayed for brevity:\r\n\r\n\r\nmpsz\r\n\r\n\r\nSimple feature collection with 323 features and 15 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\r\nProjected CRS: SVY21\r\nFirst 10 features:\r\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND\r\n1         1          1    MARINA SOUTH    MSSZ01      Y\r\n2         2          1    PEARL'S HILL    OTSZ01      Y\r\n3         3          3       BOAT QUAY    SRSZ03      Y\r\n4         4          8  HENDERSON HILL    BMSZ08      N\r\n5         5          3         REDHILL    BMSZ03      N\r\n6         6          7  ALEXANDRA HILL    BMSZ07      N\r\n7         7          9   BUKIT HO SWEE    BMSZ09      N\r\n8         8          2     CLARKE QUAY    SRSZ02      Y\r\n9         9         13 PASIR PANJANG 1    QTSZ13      N\r\n10       10          7       QUEENSWAY    QTSZ07      N\r\n        PLN_AREA_N PLN_AREA_C       REGION_N REGION_C\r\n1     MARINA SOUTH         MS CENTRAL REGION       CR\r\n2           OUTRAM         OT CENTRAL REGION       CR\r\n3  SINGAPORE RIVER         SR CENTRAL REGION       CR\r\n4      BUKIT MERAH         BM CENTRAL REGION       CR\r\n5      BUKIT MERAH         BM CENTRAL REGION       CR\r\n6      BUKIT MERAH         BM CENTRAL REGION       CR\r\n7      BUKIT MERAH         BM CENTRAL REGION       CR\r\n8  SINGAPORE RIVER         SR CENTRAL REGION       CR\r\n9       QUEENSTOWN         QT CENTRAL REGION       CR\r\n10      QUEENSTOWN         QT CENTRAL REGION       CR\r\n            INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\r\n1  5ED7EB253F99252E 2014-12-05 31595.84 29220.19   5267.381\r\n2  8C7149B9EB32EEFC 2014-12-05 28679.06 29782.05   3506.107\r\n3  C35FEFF02B13E0E5 2014-12-05 29654.96 29974.66   1740.926\r\n4  3775D82C5DDBEFBD 2014-12-05 26782.83 29933.77   3313.625\r\n5  85D9ABEF0A40678F 2014-12-05 26201.96 30005.70   2825.594\r\n6  9D286521EF5E3B59 2014-12-05 25358.82 29991.38   4428.913\r\n7  7839A8577144EFE2 2014-12-05 27680.06 30230.86   3275.312\r\n8  48661DC0FBA09F7A 2014-12-05 29253.21 30222.86   2208.619\r\n9  1F721290C421BFAB 2014-12-05 22077.34 29893.78   6571.323\r\n10 3580D2AFFBEE914C 2014-12-05 24168.31 30104.18   3454.239\r\n   SHAPE_Area                       geometry\r\n1   1630379.3 MULTIPOLYGON (((31495.56 30...\r\n2    559816.2 MULTIPOLYGON (((29092.28 30...\r\n3    160807.5 MULTIPOLYGON (((29932.33 29...\r\n4    595428.9 MULTIPOLYGON (((27131.28 30...\r\n5    387429.4 MULTIPOLYGON (((26451.03 30...\r\n6   1030378.8 MULTIPOLYGON (((25899.7 297...\r\n7    551732.0 MULTIPOLYGON (((27746.95 30...\r\n8    290184.7 MULTIPOLYGON (((29351.26 29...\r\n9   1084792.3 MULTIPOLYGON (((20996.49 30...\r\n10   631644.3 MULTIPOLYGON (((24472.11 29...\r\n\r\n2.3.2 Importing Attribute Data\r\n\r\n\r\nShow code\r\n\r\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\r\n\r\n\r\n\r\nHopefully, after this lesson, you‚Äôve remembered the main methods for importing geospatial and attribute data respectively!\r\n\r\n\r\n2.4 Data Preparation\r\n2.4.1 Data Wrangling\r\nBefore we can start mapping out our thematic map, we‚Äôll need to prepare a data table with our required values, which are as follows:\r\nYOUNG: age group 0 to 4 until age group 20 to 24\r\nECONOMY ACTIVE: age group 25-29 until age group 60-64\r\nAGED: age group 65 and above\r\nTOTAL: all age groups\r\nDEPENDENCY: the ratio of young + aged groups against the economy-active group\r\n\r\n\r\npopdata2020 <- popdata %>%\r\n  filter(Time == 2020) %>%\r\n  group_by(PA, SZ, AG) %>%\r\n  summarise(`POP` = sum(`Pop`)) %>%\r\n  ungroup()%>%\r\n  pivot_wider(names_from=AG, \r\n              values_from=POP) %>%\r\n  mutate(YOUNG = rowSums(.[3:6])\r\n         +rowSums(.[12])) %>%\r\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\r\nrowSums(.[13:15]))%>%\r\nmutate(`AGED`=rowSums(.[16:21])) %>%\r\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \r\nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\r\n/`ECONOMY ACTIVE`) %>%\r\n  select(`PA`, `SZ`, `YOUNG`, \r\n       `ECONOMY ACTIVE`, `AGED`, \r\n       `TOTAL`, `DEPENDENCY`)\r\n\r\n\r\n\r\n2.4.2 Data Wrangling Explanations\r\n\r\n\r\nHmm, there are some new functions inside here that we haven‚Äôt touched on before! Let‚Äôs explain them, one by one:\r\nfilter(): used to subset a data frame and retain rows that satisfy your conditions.\r\nIn our code, we used this to retain records inside the 2020 timeframe only.\r\n\r\ngroupby() and ungroup(): takes an existing tbl and converts it to a grouped tbl (defined by your selected variables) where operations are performed ‚Äúby group‚Äù. If our operations are complete, we‚Äôll use ungroup() to remove the grouping.\r\nFor example, for a dataset of students, I can choose to group them by sex (male/female), their grades (A/B/C) or even if they‚Äôre wearing glasses.\r\nIn our code, we our selected variables to group them by are PA, SZ AND AG, and we perform the summarise() operation on it before ungrouping.\r\n\r\nsummarise(): creates a new data frame of summary statistics based on the combination of grouping variables provided (or, if there are no grouping variables,the output will have a single row summarising all observations in the input).\r\nFor example, say that I have data on ice-cream sales. I can choose to summarise to show the average sales, or I can choose to group_by() ice-cream type (vanilla/chocolate/strawberry) and then look at the average sales across each of those types.\r\n\r\nmutate(): adds new variables and preserves existing ones.\r\nFor example, in our code, to form the DEPENDENCY column, we calculated the value of the young + aged groups over the economy-active groups and used mutate() to add it as a new column.\r\n\r\nLastly, here‚Äôs an illustrated example of pivot_wider(). It‚Äôs mean to ‚Äòwiden‚Äô data, which means increasing the number of columns and decreasing the number of rows.\r\n\r\n\r\nFrom the pivot_wider() example above, we can see the the name_from argument takes column that we want to expand the table by, while values_from refers to the column that we get the cell values from. >Note: notice that while the column names are ‚Äògone‚Äô in the pivot_wider() table, the viewer still understands what they represent!\r\nCredits to to the Youtube Channel SR for their video example on pivot_wider() üí™\r\n2.4.3 Joining the geospatial and attribute data\r\nWe‚Äôre almost ready to join the geospatial and attribute data - but wait! There‚Äôs one extra step: we‚Äôve got to convert the values in the PA and SZ fields to uppercase. This is because the values from those fields are in both uppercase and lowercase; on the other hand, SUBZONE_N and PLN_AREA_N are in uppercase only.\r\n\r\n\r\npopdata2020 <- popdata2020 %>%\r\n  mutate_at(.vars = vars(PA, SZ), \r\n          .funs = funs(toupper)) %>%\r\n  filter(`ECONOMY ACTIVE` > 0)\r\n\r\n\r\n\r\nNow, let‚Äôs join the two sets of data with left_join()! Here, we‚Äôll be using the planning subzone names (SUBZONE_N and SZ from the geospatial and attribute data frames respectively) as the common identifier.\r\n\r\n\r\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\r\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\r\n\r\n\r\n\r\nNow you might be thinking‚Ä¶ if there‚Äôs a left_join(), surely there‚Äôs a right_join() as well? What‚Äôs the difference, and why aren‚Äôt we using right_join()?\r\n\r\n\r\nleft_join(a,b) is used when dataframe ‚Äòa‚Äô is your ‚Äòmain‚Äô dataframe, and you‚Äôd like to merge some of the data from ‚Äòb‚Äô into ‚Äòa‚Äô. right_join(a,b) is vice versa: ‚Äòb‚Äô is your main, while ‚Äòa‚Äô is additional. In addition, the newly joined dataframe is modelled after your ‚Äòmain‚Äô dataframe - and since we want our output to be a simple features dataframe, we‚Äôre putting mpsz as our ‚Äòmain‚Äô one. You can find out more here!\r\n3.0 Choropleth Maps\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-09-05T14:42:04+08:00",
    "input_file": "hands-on-exercise-3.knit.md"
  },
  {
    "path": "posts/2021-08-30-hands-on-exercise-2/",
    "title": "Hands-On Exercise 02",
    "description": "Today's Adventure: Geospatial Data Wrangling with R! Using the sf package, let's learn the ropes of handling geospatial data in R: from importing to projection, we've got you covered üí™ You might even learn a geoprocessing tip or two üòÑ",
    "author": [
      {
        "name": "Megan Sim",
        "url": "https://www.linkedin.com/in/megan-sim-tze-yen/"
      }
    ],
    "date": "2021-08-30",
    "categories": [
      "Hands-On Exercise",
      "R",
      "sf",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\n1.0 Overview\r\n2.0 Setup\r\n2.1 Packages Used\r\n2.2 Data Used\r\n\r\n3.0 Importing Geospatial Data into R\r\n3.1 Importing polygon feature data in shapefile format\r\n3.2 Importing polyline feature data in shapefile form\r\n3.3 Importing GIS data in kml format\r\n\r\n4.0 Importing + Converting Aspatial Data into R\r\n4.1 Importing aspatial data\r\n4.2 Converting aspatial data\r\n4.3 Combining it together: Importing asptial data as sf\r\n\r\n5.0 Checking the Content of a Data Frame\r\n5.1 st_geometry()\r\n5.2 glimpse()\r\n5.3 head()\r\n\r\n6.0 Plotting & Projection\r\n6.1 Plotting\r\n6.2 Projection\r\n\r\n7.0 Geoprocessing\r\n7.1 Buffering\r\n7.2 Point-In-Polygon Count\r\n\r\n8.0 Exploratory Data Analysis (EDA)\r\nDIY section\r\n\r\n7.0 End Notes\r\n\r\n1.0 Overview\r\nBefore we even start, you might be wondering: why geospatial analytics? Why are we learning about it, and how is it relevant to a business or a governmental/academic institution?\r\nA quick Google search brings up thousands upon thousands of answers, articles, and research papers: I recommend Deloitte‚Äôs 3-Minute Guide on why geospatial analytics matters and the value it brings to a business its data.\r\n\r\n\r\nIn today‚Äôs Hands-On Exercise, we‚Äôll be doing an introduction to geospatial analytics in R:\r\nimporting the data and (if necessary) converting it into an accessible format\r\ncarrying out geoprocessing tasks with sf\r\ncarrying out data wrangling tasks with dplyr\r\nperforming Exploratory Data Analysis (EDA) with ggplot2\r\n2.0 Setup\r\n2.1 Packages Used\r\n\r\n\r\nThe R packages we‚Äôll be introducing today are: - sf: used for importing, managing, and processing geospatial data - tidyverse: used for importing, wrangling and visualising data (and other data science tasks!)\r\n\r\nNote: Tidyverse consists of a family of R packages, such as readr, tidyr, and dplyr - these are used for the different steps of the data wrangling + visualisation process!\r\n\r\nHere‚Äôs a handy code chunk that we‚Äôll likely be putting at the start of every file: it (a) creates a list of packages, (b) checks if they have been installed (and installs it for us if they haven‚Äôt), and lastly (c) launches them in the Rstudio environment.\r\n\r\n\r\npackages = c('sf', 'tidyverse')\r\nfor (p in packages){\r\n  if(!require(p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p,character.only = T)\r\n}\r\n\r\n\r\n\r\n2.2 Data Used\r\nThe datasets used for this exercise are:\r\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\r\nPre-Schools Location from data.gov.sg\r\nCycling Path from LTADataMall\r\nSingapore Airbnb listing data, 19 July 2021 from Inside Airbnb\r\nA good practice would be to put the data sets in a ‚Äòdata‚Äô folder, sorted accordingly into ‚Äògeospatial‚Äô and ‚Äòaspatial‚Äô folders.\r\n\r\n\r\n3.0 Importing Geospatial Data into R\r\nNow that we‚Äôve got our data, let‚Äôs take a closer look at the formats they‚Äôre in and how to import them into R. The geospatial data we have are:\r\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\r\nCyclingPath, a line feature layer in ESRI shapefile format, and\r\nPreSchool, a point feature layer in kml file format.\r\nTo import, we‚Äôll be using a handy function called st_read() from the sf package. The arguments it takes in depends on the file format. For the shapefile format, two arguments are provided: dsn to define the data path, and layer to provide the shapefile name. Bonus: we don‚Äôt need to specify the file extension for shapefiles üòÑ On the other hand, for the kml file format, our argument is the complete path with the kml file extension.\r\n\r\n\r\n3.1 Importing polygon feature data in shapefile format\r\nDataset used: MP14_SUBZONE_WEB_PL File format: shapefile Data frame type: polygon feature\r\n\r\n\r\nmpsz = st_read(dsn = \"data/geospatial\", \r\n                  layer = \"MP14_SUBZONE_WEB_PL\")\r\n\r\n\r\nReading layer `MP14_SUBZONE_WEB_PL' from data source \r\n  `C:\\Users\\megan\\relevant\\University Materials\\IS415 Geospatial Analytics & Applications\\RStudio\\IS415_msty\\_posts\\2021-08-30-hands-on-exercise-2\\data\\geospatial' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 323 features and 15 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\r\nProjected CRS: SVY21\r\n\r\nFrom the output message, we can see that in our mpsz simple feature data frame, there are 323 multipolygon features, 15 fields and is in the svy21 projected coordinates system.\r\n3.2 Importing polyline feature data in shapefile form\r\nDataset used: CyclingPath File format: shapefile Data frame type: line feature\r\n\r\n\r\ncyclingpath = st_read(dsn = \"data/geospatial\", \r\n                         layer = \"CyclingPath\")\r\n\r\n\r\nReading layer `CyclingPath' from data source \r\n  `C:\\Users\\megan\\relevant\\University Materials\\IS415 Geospatial Analytics & Applications\\RStudio\\IS415_msty\\_posts\\2021-08-30-hands-on-exercise-2\\data\\geospatial' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 3336 features and 2 fields\r\nGeometry type: MULTILINESTRING\r\nDimension:     XY\r\nBounding box:  xmin: 12831.45 ymin: 28347.98 xmax: 42799.89 ymax: 48948.15\r\nProjected CRS: SVY21\r\n\r\nFrom the output message, we can see that in our cyclingpath linestring feature data frame, there are 1625 linestring features, 2 fields and is in the svy21 projected coordinates system.\r\n3.3 Importing GIS data in kml format\r\nDataset used: pre-schools-location-kml File format: kml Data frame type: point feature\r\n\r\n\r\npreschool = st_read(\"data/geospatial/pre-schools-location-kml.kml\")\r\n\r\n\r\nReading layer `PRESCHOOLS_LOCATION' from data source \r\n  `C:\\Users\\megan\\relevant\\University Materials\\IS415 Geospatial Analytics & Applications\\RStudio\\IS415_msty\\_posts\\2021-08-30-hands-on-exercise-2\\data\\geospatial\\pre-schools-location-kml.kml' \r\n  using driver `KML'\r\nSimple feature collection with 1925 features and 2 fields\r\nGeometry type: POINT\r\nDimension:     XYZ\r\nBounding box:  xmin: 103.6824 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\r\nz_range:       zmin: 0 zmax: 0\r\nGeodetic CRS:  WGS 84\r\n\r\nFrom the output message, we can see that in our preschool point feature data frame, there are 1359 linestring features, 2 fields and is in the wgs84 projected coordinates system.\r\n4.0 Importing + Converting Aspatial Data into R\r\nFor aspatial data, such as the listings Airbnb datset, there‚Äôs an extra step in the importing process. We‚Äôll import it into a tibble data frame, then convert it into a simple feature data frame.\r\n4.1 Importing aspatial data\r\nSince our listings data set is in a csv file format, we‚Äôll use the read_csv() function from the readr package, like so:\r\n\r\n\r\nlistings <- read_csv(\"data/aspatial/listings.csv\")\r\nglimpse(listings) \r\n\r\n\r\nRows: 4,252\r\nColumns: 16\r\n$ id                             <dbl> 50646, 71609, 71896, 71903, 2~\r\n$ name                           <chr> \"Pleasant Room along Bukit Ti~\r\n$ host_id                        <dbl> 227796, 367042, 367042, 36704~\r\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belind~\r\n$ neighbourhood_group            <chr> \"Central Region\", \"East Regio~\r\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"T~\r\n$ latitude                       <dbl> 1.33432, 1.34537, 1.34754, 1.~\r\n$ longitude                      <dbl> 103.7852, 103.9589, 103.9596,~\r\n$ room_type                      <chr> \"Private room\", \"Private room~\r\n$ price                          <dbl> 80, 178, 81, 81, 52, 40, 72, ~\r\n$ minimum_nights                 <dbl> 90, 90, 90, 90, 14, 14, 90, 8~\r\n$ number_of_reviews              <dbl> 18, 20, 24, 48, 20, 13, 133, ~\r\n$ last_review                    <date> 2014-07-08, 2019-12-28, 2014~\r\n$ reviews_per_month              <dbl> 0.22, 0.28, 0.33, 0.67, 0.20,~\r\n$ calculated_host_listings_count <dbl> 1, 4, 4, 4, 50, 50, 7, 1, 50,~\r\n$ availability_365               <dbl> 365, 365, 365, 365, 353, 364,~\r\n\r\nFrom the output message, we can see that in our listing tibble data frame, there are 4252 rows and 16 columns (not features and fields like in our simple data feature frame!) Take note of the latitude and longitude fields - we‚Äôll be using them in the next phase.\r\n\r\nAssumption: The data is in the wgs84 Geographic Coordinate System on account of its latitude/longtitude fields.\r\n\r\n4.2 Converting aspatial data\r\nNow, let‚Äôs convert our listing tibble data frame into a by using the st_as_sf() function from the sf package.\r\n\r\n\r\nlistings_sf <- st_as_sf(listings, \r\n                       coords = c(\"longitude\", \"latitude\"),\r\n                       crs=4326) %>%\r\n  st_transform(crs = 3414)\r\n\r\n\r\n\r\nLet‚Äôs explain the code chunk!\r\n\r\n\r\nThis gives us the new simple feature data frame, listings_sf:\r\n\r\n\r\nglimpse(listings_sf)\r\n\r\n\r\nRows: 4,252\r\nColumns: 15\r\n$ id                             <dbl> 50646, 71609, 71896, 71903, 2~\r\n$ name                           <chr> \"Pleasant Room along Bukit Ti~\r\n$ host_id                        <dbl> 227796, 367042, 367042, 36704~\r\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belind~\r\n$ neighbourhood_group            <chr> \"Central Region\", \"East Regio~\r\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"T~\r\n$ room_type                      <chr> \"Private room\", \"Private room~\r\n$ price                          <dbl> 80, 178, 81, 81, 52, 40, 72, ~\r\n$ minimum_nights                 <dbl> 90, 90, 90, 90, 14, 14, 90, 8~\r\n$ number_of_reviews              <dbl> 18, 20, 24, 48, 20, 13, 133, ~\r\n$ last_review                    <date> 2014-07-08, 2019-12-28, 2014~\r\n$ reviews_per_month              <dbl> 0.22, 0.28, 0.33, 0.67, 0.20,~\r\n$ calculated_host_listings_count <dbl> 1, 4, 4, 4, 50, 50, 7, 1, 50,~\r\n$ availability_365               <dbl> 365, 365, 365, 365, 353, 364,~\r\n$ geometry                       <POINT [m]> POINT (22646.02 35167.9~\r\n\r\n\r\nNote that a new column called geometry has been added! In addition, longtitude and latitude have both been dropped.\r\n\r\n4.3 Combining it together: Importing asptial data as sf\r\n5.0 Checking the Content of a Data Frame\r\nNow that we‚Äôve got our data frames, this begs the question: what exactly is inside them? Let‚Äôs learn how to retrieve the information of the dataframe with 3 simple methods!\r\n5.1 st_geometry()\r\nLet‚Äôs say that we want a preliminary look at our data - just seeing the basic feature information is sufficient. In this case, st_geometry() is most appropriate:\r\n\r\n\r\nst_geometry(mpsz)\r\n\r\n\r\nGeometry set for 323 features \r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\r\nProjected CRS: SVY21\r\nFirst 5 geometries:\r\n\r\nAs we can see, only the basic information of the feature class (type of geometry, geographic extent of features and CRS) is displayed.\r\n5.2 glimpse()\r\nHowever, basic information won‚Äôt take us very far. Let‚Äôs dig a little deeper and learn about the associated attribute information in the data frame. This is where glimpse() comes in:\r\n\r\n\r\nglimpse(mpsz)\r\n\r\n\r\nRows: 323\r\nColumns: 16\r\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15~\r\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1,~\r\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HEN~\r\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\",~\r\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\",~\r\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUK~\r\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"~\r\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGI~\r\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"~\r\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF0~\r\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, ~\r\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96,~\r\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70,~\r\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594,~\r\n$ SHAPE_Area <dbl> 1630379.3, 559816.2, 160807.5, 595428.9, 387429.4~\r\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULT~\r\n\r\nWith glimpse(), we now know the data type of each field - for example, FMEL-UPD_D field is of the date data type.\r\n5.3 head()\r\nSometimes, we want to dig as deep as we can and reveal all the information of a feature object. In this case, head() is best:\r\n\r\n\r\nhead(mpsz)\r\n\r\n\r\nSimple feature collection with 6 features and 15 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 24468.89 ymin: 28369.47 xmax: 32362.39 ymax: 30542.74\r\nProjected CRS: SVY21\r\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\r\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\r\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\r\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\r\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\r\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\r\n6        6          7 ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\r\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D\r\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05\r\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05\r\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05\r\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05\r\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05\r\n6         BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05\r\n    X_ADDR   Y_ADDR SHAPE_Leng SHAPE_Area\r\n1 31595.84 29220.19   5267.381  1630379.3\r\n2 28679.06 29782.05   3506.107   559816.2\r\n3 29654.96 29974.66   1740.926   160807.5\r\n4 26782.83 29933.77   3313.625   595428.9\r\n5 26201.96 30005.70   2825.594   387429.4\r\n6 25358.82 29991.38   4428.913  1030378.8\r\n                        geometry\r\n1 MULTIPOLYGON (((31495.56 30...\r\n2 MULTIPOLYGON (((29092.28 30...\r\n3 MULTIPOLYGON (((29932.33 29...\r\n4 MULTIPOLYGON (((27131.28 30...\r\n5 MULTIPOLYGON (((26451.03 30...\r\n6 MULTIPOLYGON (((25899.7 297...\r\n\r\n\r\nNote that the ‚Äòn‚Äô argument allows us to select how many records to display!\r\n\r\n6.0 Plotting & Projection\r\n6.1 Plotting\r\nNow that we‚Äôve got our data, we get into the meat of the matter: visualisation! Having a dataframe of fields and data points is taxing for human eyes to process, but with a bit of plotting and mapping, our geospatial data becomes a gold mine of great insights. Let‚Äôs try to plot() it out:\r\n\r\n\r\nplot(mpsz)\r\n\r\n\r\n\r\n\r\nThis is a handy multi-plot of all attributes (up to a reasonable amount). But maybe we just want to plot a specific attribute:\r\n\r\n\r\nplot(mpsz[\"PLN_AREA_N\"])\r\n\r\n\r\n\r\n\r\nThere are also times where we need just the geometry - in other words, the map outline:\r\n\r\n\r\nplot(st_geometry(mpsz))\r\n\r\n\r\n\r\n\r\n6.2 Projection\r\nProjection transformation is when we project a simple feature data frame from one coordinate system to another. There are two common issues that require projection transformation: (a) missing or inaccurate coordinate system, or (b) inappropriate coordinate systems for the analysis chosen.\r\n6.2.1 Missing/Inaccurate Coordinate System\r\nTo tackle the first issue, we‚Äôll need to assign an ESPG code to the data frame. Firstly, let‚Äôs check the coordinate system of our mpsz data frame:\r\n\r\n\r\nst_crs(mpsz)\r\n\r\n\r\nCoordinate Reference System:\r\n  User input: SVY21 \r\n  wkt:\r\nPROJCRS[\"SVY21\",\r\n    BASEGEOGCRS[\"SVY21[WGS84]\",\r\n        DATUM[\"World Geodetic System 1984\",\r\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\r\n                LENGTHUNIT[\"metre\",1]],\r\n            ID[\"EPSG\",6326]],\r\n        PRIMEM[\"Greenwich\",0,\r\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\r\n    CONVERSION[\"unnamed\",\r\n        METHOD[\"Transverse Mercator\",\r\n            ID[\"EPSG\",9807]],\r\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\r\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\r\n            ID[\"EPSG\",8801]],\r\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\r\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\r\n            ID[\"EPSG\",8802]],\r\n        PARAMETER[\"Scale factor at natural origin\",1,\r\n            SCALEUNIT[\"unity\",1],\r\n            ID[\"EPSG\",8805]],\r\n        PARAMETER[\"False easting\",28001.642,\r\n            LENGTHUNIT[\"metre\",1],\r\n            ID[\"EPSG\",8806]],\r\n        PARAMETER[\"False northing\",38744.572,\r\n            LENGTHUNIT[\"metre\",1],\r\n            ID[\"EPSG\",8807]]],\r\n    CS[Cartesian,2],\r\n        AXIS[\"(E)\",east,\r\n            ORDER[1],\r\n            LENGTHUNIT[\"metre\",1,\r\n                ID[\"EPSG\",9001]]],\r\n        AXIS[\"(N)\",north,\r\n            ORDER[2],\r\n            LENGTHUNIT[\"metre\",1,\r\n                ID[\"EPSG\",9001]]]]\r\n\r\nAlthough mpsz data frame is projected in svy21, the output indicates that the EPSG is 9001. This is the wrong EPSG code! The correct EPSG code for svy21 should be 3414. Let‚Äôs change it:\r\n\r\n\r\nmpsz3414 <- st_set_crs(mpsz, 3414)\r\nst_crs(mpsz3414)\r\n\r\n\r\nCoordinate Reference System:\r\n  User input: EPSG:3414 \r\n  wkt:\r\nPROJCRS[\"SVY21 / Singapore TM\",\r\n    BASEGEOGCRS[\"SVY21\",\r\n        DATUM[\"SVY21\",\r\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\r\n                LENGTHUNIT[\"metre\",1]]],\r\n        PRIMEM[\"Greenwich\",0,\r\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\r\n        ID[\"EPSG\",4757]],\r\n    CONVERSION[\"Singapore Transverse Mercator\",\r\n        METHOD[\"Transverse Mercator\",\r\n            ID[\"EPSG\",9807]],\r\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\r\n            ANGLEUNIT[\"degree\",0.0174532925199433],\r\n            ID[\"EPSG\",8801]],\r\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\r\n            ANGLEUNIT[\"degree\",0.0174532925199433],\r\n            ID[\"EPSG\",8802]],\r\n        PARAMETER[\"Scale factor at natural origin\",1,\r\n            SCALEUNIT[\"unity\",1],\r\n            ID[\"EPSG\",8805]],\r\n        PARAMETER[\"False easting\",28001.642,\r\n            LENGTHUNIT[\"metre\",1],\r\n            ID[\"EPSG\",8806]],\r\n        PARAMETER[\"False northing\",38744.572,\r\n            LENGTHUNIT[\"metre\",1],\r\n            ID[\"EPSG\",8807]]],\r\n    CS[Cartesian,2],\r\n        AXIS[\"northing (N)\",north,\r\n            ORDER[1],\r\n            LENGTHUNIT[\"metre\",1]],\r\n        AXIS[\"easting (E)\",east,\r\n            ORDER[2],\r\n            LENGTHUNIT[\"metre\",1]],\r\n    USAGE[\r\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\r\n        AREA[\"Singapore - onshore and offshore.\"],\r\n        BBOX[1.13,103.59,1.47,104.07]],\r\n    ID[\"EPSG\",3414]]\r\n\r\n6.2.2 Inappropriate Coordinate Systems\r\nStill remember our geospatial data overview in 3.0? You might have noticed that the coordinate system differed among our data frames: mpsz and cyclingpath are svy21, while preschool is wgs84. preschool might run into issues when we‚Äôre performing geoprocessing, because a geographic coordinate system is not appropriate if our analysis needs distance or/and area measurements.\r\n\r\n\r\npreschool3414 <- st_transform(preschool, \r\n                              crs = 3414)\r\nst_geometry(preschool3414)\r\n\r\n\r\nGeometry set for 1925 features \r\nGeometry type: POINT\r\nDimension:     XYZ\r\nBounding box:  xmin: 11203.01 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\r\nz_range:       zmin: 0 zmax: 0\r\nProjected CRS: SVY21 / Singapore TM\r\nFirst 5 geometries:\r\n\r\nNotice the change to the svy21 projected coordinate system!\r\n\r\nYou might notice that the values in the bounding box have changed, too! They are now greater than the original 0-360 range of decimal degree commonly used by majority of the geographic coordinate systems.\r\n\r\n7.0 Geoprocessing\r\nOther than handling data, the handy sf package also has functions for geoprocessing (otherwise known as GIS analysis). Let‚Äôs explore some commonly-used geoprocessingn functions!\r\n7.1 Buffering\r\nLet‚Äôs say that in a bid to promote healthier living, the authorities are planning to upgrade the existing cycling paths, making it safer and more appealing to cyclists all around! To do that, they‚Äôll need to acquire 5 metres of reserved land on both sides of the cycling path. Our task is to find the total area of land that‚Äôs to be acquired.\r\nFirstly, let‚Äôs compute the 5-meter buffers around the cycling paths:\r\n\r\n\r\nbuffer_cycling <- st_buffer(cyclingpath, \r\n                               dist=5, nQuadSegs = 30)\r\n\r\n\r\n\r\nThen, let‚Äôs actually calculate the area of the buffers:\r\n\r\n\r\nbuffer_cycling$AREA <- st_area(buffer_cycling)\r\n\r\n\r\n\r\nLastly, let‚Äôs find the total area with sum():\r\n\r\n\r\nsum(buffer_cycling$AREA)\r\n\r\n\r\n1642750 [m^2]\r\n\r\n7.2 Point-In-Polygon Count\r\nLet‚Äôs say that a preschool service group is organising a nation-wide event, and wants to find out the number of preschools in each Planning Subzone.\r\nWe can kill two birds with one stone with this handy combination of st_intersects() and length(). st_intersects() helps us identify the preschools located in each Planningn Subzone, while length() calculates the number of preschools per Planning Subzone.\r\n\r\n\r\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\r\n\r\n\r\n\r\nLet‚Äôs check a summary of PreSch Count:\r\n\r\n\r\nsummary(mpsz3414$`PreSch Count`)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n   0.00    0.00    3.00    5.96    9.00   58.00 \r\n\r\nNow, the preschool service group wants to hold a subzone-specific event! They want to target the Planning Subzone with the most number of preschools to maximise their outreach, so let‚Äôs help them out with top_n():\r\n\r\n\r\ntop_n(mpsz3414, 1, `PreSch Count`)\r\n\r\n\r\nSimple feature collection with 1 feature and 16 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\r\nProjected CRS: SVY21 / Singapore TM\r\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N\r\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES\r\n  PLN_AREA_C    REGION_N REGION_C          INC_CRC FMEL_UPD_D\r\n1         TM EAST REGION       ER 21658EAAF84F4D8D 2014-12-05\r\n    X_ADDR   Y_ADDR SHAPE_Leng SHAPE_Area\r\n1 41122.55 37392.39   10180.62    4339824\r\n                        geometry PreSch Count\r\n1 MULTIPOLYGON (((42196.76 38...           58\r\n\r\nLastly, the preschool service group wants to know the density of preschools by planning subzone for future events. We‚Äôll need to derive the area of each planning subzone:\r\n\r\n\r\nmpsz3414$Area <- mpsz3414 %>%\r\n  st_area()\r\n\r\n\r\n\r\nNow, introducing a new function, mutate()! It‚Äôs to help us add the density column into mpsz3414:\r\n\r\n\r\nmpsz3414 <- mpsz3414 %>%\r\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\r\n\r\n\r\n\r\n\r\nWhile they look similar and have a similar function (to add new variables), there‚Äôs a difference between mutate() and transmute()! mutate() adds new variables and preserves existing ones while transmute() adds new variables and drops existing ones. Use the appropriate one for the situation!\r\n\r\n8.0 Exploratory Data Analysis (EDA)\r\nWhy do we do EDA? Here‚Äôs a great Medium article that introduces EDA: from its significance to its components, it‚Äôs got you covered!\r\n\r\n\r\nFor this EDA section, we‚Äôll be introducing various ggplot2 functions that‚Äôll help us create functional and yet truthful statistical graphs that‚Äôll help us visualise and understand our data!\r\nFirstly, let‚Äôs use hist() to plot a histogram and reveal the distribution of PreSch Density:\r\n\r\n\r\nhist(mpsz3414$`PreSch Density`)\r\n\r\n\r\n\r\n\r\nThat looks good - but it‚Äôs not something you‚Äôll publish in your reports! Let‚Äôs add a little customisation:\r\n\r\n\r\nggplot(data=mpsz3414, \r\n       aes(x= as.numeric(`PreSch Density`)))+\r\n  geom_histogram(bins=20, \r\n                 color=\"black\", \r\n                 fill=\"light blue\") +\r\n  labs(title = \"Are pre-school even distributed in Singapore?\",\r\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\r\n      x = \"Pre-school density (per km sq)\",\r\n      y = \"Frequency\")\r\n\r\n\r\n\r\n\r\nDIY section\r\nDIY: Using ggplot2, plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\r\nLet‚Äôs approach this step-by-step. Firstly, we‚Äôll need the variables Pre-school Density and Pre-school Count. Luckily, we‚Äôve already created both in 2.5.2 Point-In-Polygon Count.\r\nNext, we need to plot a scatterplot. For our histogram, we used geom_histogram - and for a scatterplot, we‚Äôll use geom_point. We can further customise this by adjusting the colour, size and fill of the points, like so:\r\n\r\n\r\nggplot(data=mpsz3414, \r\n       aes(y = `PreSch Count`, \r\n           x= as.numeric(`PreSch Density`)))+\r\n  geom_point(color=\"black\", \r\n             fill=\"light blue\") + \r\n  labs(title = \"\",\r\n      x = \"Pre-school density (per km sq)\",\r\n      y = \"Pre-school count\")\r\n\r\n\r\n\r\n\r\nSeem finished? There‚Äôs actually one more step: ensuring that your data is presented as fairly as possible - we don‚Äôt want to mislead our readers üò± In this case, notice that the aspect ratio of our graph is off - its height is greater than its width! Let‚Äôs correct that by setting limiters with the xlim() and ylim() functions inside ggplot2:\r\n\r\n\r\n\r\nAnd there we have it! A representative scatterplot visualisation ‚ú®\r\n7.0 End Notes\r\nThis is the end of this week‚Äôs exercise! I‚Äôm really excited to learn more about geospatial analytics: we‚Äôve just begun to scratch the surface, and there‚Äôs so much more to explore and experiment with!\r\nOn a related sidenote: I‚Äôve learned a number of things myself when creating this blog post! For example, I wanted to have a table of contents, but googling ‚Äòsidebar navigation pane‚Äô proved to be far less effective than just reading the Distill Basics - which is a lesson learnt on reading the provided materials!\r\nOn a funny note, here‚Äôs a mini-comic on my experience with tabsets:\r\n\r\n\r\nTabsets are meant for html documents, but this output is a distill article‚Ä¶ Of course it didn‚Äôt work üò≠ Well, that‚Äôs just another learning point in our Geospatial Analytics Journey !\r\nWalk with me as we dive deeper the coming weeks! ü§ó üíñ ‚ú®\r\nP.S. I found my old tablet so I could illustrate the points better! Handwriting will be more consistent from this blog post onwards üòÑ\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-09-02T17:48:14+08:00",
    "input_file": "hands-on-exercise-2.knit.md"
  },
  {
    "path": "posts/2021-08-23-in-class-exercise-2/",
    "title": "In-Class Exercise 02",
    "description": "This is a dummy blog post for testing out the various ins and outs of a Distill blog. In this in-class exercise, I learned how to handle geospatial data in R by using the sf package.",
    "author": [
      {
        "name": "Megan Sim",
        "url": "https://www.linkedin.com/in/megan-sim-tze-yen/"
      }
    ],
    "date": "2021-08-23",
    "categories": [],
    "contents": "\r\nGetting Started\r\nThis code chunk performs three tasks:\r\nA packaging list call packages will be created. It consists of all the R packages required to accomplish this hands-on exercise.\r\nNext, the code chunk will check if the R packages in packages have been installed in R. If they have yet been installed, they will be installed.\r\nAfter all the R packages have been installed, they will be launched in RStudio environment.\r\n\r\n\r\npackages <- c('sf', 'tidyverse')\r\nfor(p in packages){\r\n  if(!require(p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-09-02T15:09:30+08:00",
    "input_file": "in-class-exercise-2.knit.md"
  }
]
